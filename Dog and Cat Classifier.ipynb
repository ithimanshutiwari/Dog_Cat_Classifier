{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Necessary Packages\n",
    "1. Tensorflow for Machine Learning Functions\n",
    "2. Numpy for numerical computation\n",
    "3. os for dealing with directory\n",
    "4. cv2 and plt for reading and plotting image\n",
    "5. skimage for image resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os,random,cv2               \n",
    "import matplotlib.pyplot as plt    \n",
    "from skimage import transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Supress the warning\n",
    "By overriding warn function with manual empty function call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Training and Testing Dataset\n",
    " Returns the list of filename present in given directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "def listFiles(path):\n",
    "    dirs = os.listdir(path)\n",
    "    filelists = []\n",
    "    for i in dirs:\n",
    "        filelists.append(os.path.join(path,i))        \n",
    "    return filelists\n",
    "\n",
    "def read_dataset(filelists,batch_size):\n",
    "    imagedata = np.zeros((batch_size,49152),dtype=np.float32)                         \n",
    "    labels = np.zeros((batch_size,2),dtype=np.float32)\n",
    "    global start\n",
    "    \n",
    "    if (start+1)*batch_size > len(filelists):\n",
    "        random.shuffle(filelists)\n",
    "        start = 0\n",
    "        \n",
    "    for i in range(start*batch_size,(start+1)*batch_size):\n",
    "        img = cv2.imread(filelists[i])        \n",
    "        img = transform.resize(img,(128,128,3))\n",
    "        img = img.reshape((1,49152))      \n",
    "               \n",
    "        st = filelists[i].find(\"cat\")\n",
    "        if(st != -1):\n",
    "            label = [1,0]  # CAT\n",
    "            labels[i-start*batch_size]=label\n",
    "        else:\n",
    "            label = [0,1]  # DOG\n",
    "            labels[i-start*batch_size]=label\n",
    "            \n",
    "        imagedata[i-start*batch_size]=img       \n",
    "        \n",
    "    start = start+1   \n",
    "    return imagedata.reshape((batch_size,49152)),labels.reshape((batch_size,2))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining CONV and FC Layers\n",
    "Convolution layer does feature extraction and fully connected layer does the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(input,size_in,size_out,name=\"conv\"):\n",
    "    w = tf.Variable(tf.truncated_normal([5, 5, size_in, size_out], stddev=0.1), name=\"W\")\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"B\")\n",
    "    conv = tf.nn.conv2d(input, w, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "    act = tf.nn.relu(conv + b)\n",
    "    return tf.nn.max_pool(act, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "def fc_layer(input,size_in,size_out,name=\"fc\"):\n",
    "    w = tf.Variable(tf.truncated_normal([size_in, size_out], stddev=0.1), name=\"W\")\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"B\")\n",
    "    act = tf.matmul(input, w) + b  \n",
    "    return act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a CNN Architecture\n",
    "It describes how many layers are there in your model, how they are connected, what is kernel size, extent of kernel overlapping, types of activation and pooling  and type of optimizer and loss etc.\n",
    "\n",
    "It consists 2 convolution and 1 fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN():\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 49152], name=\"x\")\n",
    "    x_image = tf.reshape(x, [-1, 128, 128, 3])\n",
    "    y = tf.placeholder(tf.float32, shape=[None, 2], name=\"labels\")\n",
    "    \n",
    "    conv1 = conv_layer(x_image, 3, 32, \"conv1\")\n",
    "    conv2 = conv_layer(conv1, 32, 64, \"conv2\")\n",
    "    \n",
    "    \n",
    "    flatten = tf.reshape(conv2,[-1,32*32*64])    \n",
    "    logits = fc_layer(flatten,32*32*64,2,\"fc1\")\n",
    "    \n",
    "    xent = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y), name=\"xent\")   \n",
    "    train = tf.train.AdamOptimizer(0.001).minimize(xent)\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    \n",
    "    filelists = listFiles(\"./train/\")  # It consists nearly 22K images for training\n",
    "    for i in range(250*10):            # 10 is epoch size\n",
    "        if i%250==0:\n",
    "            print(str(i)+\" epoch is running\")\n",
    "        x_train,y_train = read_dataset(filelists,100)    # 100 is batch size    \n",
    "        sess.run(train,feed_dict={x:x_train, y: y_train})\n",
    "        \n",
    "    \n",
    "\n",
    "    global start    \n",
    "    start = 0  \n",
    "    filelists = listFiles(\"./test/\")   # It contains nearly 3K images for testing\n",
    "    random.shuffle(filelists)\n",
    "    for i in range(12):  \n",
    "        x_test,y_test = read_dataset(filelists,250)\n",
    "        [a,b,c] = sess.run([train,accuracy,logits],feed_dict={x:x_test,y:y_test})\n",
    "        print(b)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing and Testing Accuracy\n",
    "It means calling the function and checking how much accuracy we are getting. When I test it on 250 size batch, it gives me accuracy of around 75-80%. The mean goes around 78%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
