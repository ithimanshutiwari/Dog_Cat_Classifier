{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os,random,cv2               \n",
    "import matplotlib.pyplot as plt    # For Plotting and viewing images\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "start = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Training and Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listFiles(path):\n",
    "    dirs = os.listdir(path)\n",
    "    filelists = []\n",
    "    for i in dirs:\n",
    "        filelists.append(os.path.join(path,i))        \n",
    "    return filelists\n",
    "\n",
    "def read_dataset(filelists,batch_size):\n",
    "    imagedata = np.zeros((batch_size,128,128,3),dtype=np.float32)                         \n",
    "    labels = np.zeros((batch_size,2),dtype=np.float32)\n",
    "    global start\n",
    "    \n",
    "    if (start+1)*batch_size > len(filelists):\n",
    "        random.shuffle(filelists)\n",
    "        start = 0\n",
    "        \n",
    "    for i in range(start*batch_size,(start+1)*batch_size):\n",
    "        img = cv2.imread(filelists[i])\n",
    "        img = np.resize(img,(128,128,3))\n",
    "        img = np.reshape(img,(-1,128,128,3))\n",
    "\n",
    "        index = filelists[i].find(\"cat\")\n",
    "        if(index!=-1):\n",
    "            label = [1.0,0.0]  # CAT  DOG\n",
    "        else:\n",
    "            label = [0.0,1.0]\n",
    "        \n",
    "        imagedata[i-start*batch_size]=img\n",
    "        labels[i-start*batch_size]=label\n",
    "        \n",
    "    start = start+1   \n",
    "    return imagedata.reshape((batch_size,49152)),labels.reshape((batch_size,2))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining CONV and FC Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 is running.\n",
      "Iteration 1 is running.\n",
      "Iteration 2 is running.\n",
      "Iteration 3 is running.\n",
      "Iteration 4 is running.\n",
      "Iteration 5 is running.\n",
      "Iteration 6 is running.\n",
      "Iteration 7 is running.\n",
      "Iteration 8 is running.\n",
      "Iteration 9 is running.\n",
      "Iteration 10 is running.\n",
      "Iteration 11 is running.\n",
      "Iteration 12 is running.\n",
      "Iteration 13 is running.\n",
      "Iteration 14 is running.\n",
      "Iteration 15 is running.\n",
      "Iteration 16 is running.\n",
      "Iteration 17 is running.\n",
      "Iteration 18 is running.\n",
      "Iteration 19 is running.\n",
      "Iteration 20 is running.\n",
      "Iteration 21 is running.\n",
      "Iteration 22 is running.\n",
      "Iteration 23 is running.\n",
      "Iteration 24 is running.\n",
      "Iteration 25 is running.\n",
      "Iteration 26 is running.\n",
      "Iteration 27 is running.\n",
      "Iteration 28 is running.\n",
      "Iteration 29 is running.\n",
      "Iteration 30 is running.\n",
      "Iteration 31 is running.\n",
      "Iteration 32 is running.\n"
     ]
    }
   ],
   "source": [
    "def conv_layer(input_img,size_in,size_out,name=\"conv\"):\n",
    "    W = tf.Variable(tf.random_normal([3,3,size_in,size_out]),dtype=tf.float32,name=\"Weight\")\n",
    "    b = tf.Variable(tf.constant([size_out],tf.float32,name=\"Bias\"))\n",
    "    f_map = tf.nn.conv2d(input_img,W,strides=[1,1,1,1],padding=\"SAME\")\n",
    "    f_map2 = tf.nn.relu(f_map+b)\n",
    "    return tf.nn.max_pool(f_map2,ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"SAME\")\n",
    "\n",
    "def fc_layer(input_img,size_in,size_out,name=\"fc\"):\n",
    "    W = tf.Variable(tf.random_normal([size_in,size_out]),dtype=tf.float32,name=\"Weight\")\n",
    "    b = tf.Variable(tf.constant([size_out],tf.float32,name=\"Bias\"))\n",
    "    #In[0]: [32768,2], In[1]: [100,32768]\n",
    "    \n",
    "    f_map = tf.matmul(input_img,W)+b    \n",
    "    f_map2 = tf.nn.relu(f_map)    \n",
    "    return f_map2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model():\n",
    "    x = tf.placeholder(tf.float32,(None,49152))\n",
    "    x_image = tf.reshape(x,[-1,128,128,3])\n",
    "    y_true = tf.placeholder(tf.float32,(None,2)) # one-hot encoding\n",
    "    \n",
    "    conv1 = conv_layer(x_image,3,16,\"conv1\")\n",
    "    conv2 = conv_layer(conv1,16,32,\"conv2\")\n",
    "    \n",
    "    \n",
    "    (d,a,b,c) = conv2.shape\n",
    "    flatten = tf.reshape(conv2,[-1,a*b*c])\n",
    "    \n",
    "    fc1 = fc_layer(flatten,32768,2,\"fc1\")\n",
    "    \n",
    "    xent = tf.nn.softmax_cross_entropy_with_logits(logits=fc1,labels=y_true)\n",
    "    loss = tf.reduce_mean(xent)    \n",
    "    train = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "    \n",
    "    correct_pred = tf.equal(tf.argmax(y_true,1),tf.argmax(fc1,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    \n",
    "    filelists = listFiles(\"./train/\")\n",
    "    for i in range(250*50):\n",
    "        print(\"Iteration \"+str(i)+\" is running.\")\n",
    "        x_train,y_train = read_dataset(filelists,100)\n",
    "        sess.run(train,feed_dict={x:x_train, y_true: y_train})\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "    filelists = listFiles(\"./train/\")\n",
    "    start = 0  \n",
    "    x_test,y_test = read_dataset(filelists,len(filelists))\n",
    "    accuracy = sess.run(accuracy,feed_dict={x:x_test,y_true:y_test})\n",
    "    \n",
    "    print(\"Accuracy is %f\"%(accuracy)) \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executing and Testing Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
